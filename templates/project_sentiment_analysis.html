<div class="container">

    <div class="accordion" id="frenchSentimentAnalysis">
        <div class="accordion-item">
            <h1 class="accordion-header" id="heading-1">
                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#introduction" aria-expanded="true" aria-controls="introduction" style="background-color:#7C81AD;">
                   Introduction
                </button>
            </h1>
            <div id="introduction" class="accordion-collapse collapse show" aria-labelledby="heading-1" data-bs-parent="#frenchSentimentAnalysis">
                <div class="accordion-body">
                 
                    <p>In this article, we will embark on an extensive project involving deep learning and web development. Our objective is to build a model capable of categorizing French movie reviews into classes (true or false) if the review is Positive or Negative. Following the model's training, we will integrate it into a user-friendly interface (UI) and guide you through its real-world implementation.</p>
                
                    <p>Our approach involves creating a straightforward website where users can input sentence and determine it respective category. We will demonstrate how to adapt the code to accommodate various dataset formats and models. To train the model, we will leverage Python libraries and packages such as PyTorch and NumPy.</p>
                
                    <p>For the backend, we will utilize FastAPI to create APIs, while HTML, CSS, Bootstrap, JavaScript, and jQuery will be employed for the UI development. As a comprehensive project, we will also incorporate DevOps tools like Git, Docker, and GitHub.</p>
                
                    <p>In the final stages, we will provide instructions on deploying this website on Heroku.</p>
                </div>

            </div>
        </div>

        <div class="accordion-item">
            <h1 class="accordion-header" id="heading-0">
                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#configuration" aria-expanded="true" aria-controls="configuration" style="background-color:#7C81AD;">
                    Configuration and Data Path Management for Sentiment Analysis
                </button>
            </h1>
            <div id="configuration" class="accordion-collapse collapse show" aria-labelledby="heading-0" data-bs-parent="#frenchSentimentAnalysis">
                <div class="accordion-body">
                    <h3>Introduction</h3>
                    <p>Before embarking on any data analysis or machine learning project, proper configuration and data path management are essential. In this article, we delve into a Python script that centralizes configuration settings and data paths for a sentiment analysis project. This script streamlines the process, making it easy to adjust parameters and file paths for your specific project.</p>
                 
                    <h3>Code explanation</h3>
<pre>
<code class="language-python">
import torch
</code>
</pre>

                    <p>The script begins by importing the PyTorch library, which will be used for deep learning tasks.</p>
<pre>
<code class="language-python">
TRAIN_DATA_PATH = "./Raw_Data/train.csv"
VAL_DATA_PATH = "./Raw_Data/valid.csv"
TEST_DATA_PATH = "./Raw_Data/test.csv"
    
</code>
</pre>

<p>These lines define the paths to the training, validation, and test data files. It's essential to specify the correct paths to your data files for the sentiment analysis project.</p>
 
<pre>
<code class="language-python">
PRETRAINED_MODEL_PATH = "camembert-base"
</code>
</pre>
<p>
This variable stores the path to the pre-trained Camembert model. It's crucial for leveraging pre-trained models for various NLP tasks.</p>^
<pre>
<code class="language-python">
MAX_LENGTH = 130
</code>
</pre>

<p>The MAX_LENGTH variable sets the maximum length for input sequences. It's essential for data preprocessing and model compatibility.</p>

<pre>
<code class="language-python">
BATCH_SIZE = 8
</code>
</pre>

<p>The BATCH_SIZE variable defines the batch size for data loading. The appropriate batch size depends on your hardware and the size of your dataset.</p>

<pre>
<code class="language-python">
RESULT_PATH = "simple_test.txt"
</code>
</pre>
<p>RESULT_PATH specifies the file path for storing the results or output of your sentiment analysis, such as classification reports or predictions.</p>
<pre>
<code class="language-python">
FINAL_TRAIN_MODEL_PATH = "FINAL_TRAIN_MODEL.h5"
</code>
</pre>

<p>FINAL_TRAIN_MODEL_PATH stores the path where the final trained model will be saved. It's crucial for model persistence and future use.</p>

<pre>
    <code class="language-python">
def device_GPU_CPU():
    # ... (see next explanation)
    </code>
    </pre>

<p>The device_GPU_CPU function is designed to detect and select the appropriate device (GPU or CPU) for training the sentiment analysis model. It checks for the availability of GPUs and provides information about the available GPUs.</p>
<h3>Conclusion</h3>
<p>Efficient configuration and data path management are the foundations of a successful sentiment analysis project. By centralizing these settings and data paths in a dedicated script, you ensure flexibility and ease of adjustment for different datasets and projects. This script, along with proper configuration, sets the stage for your sentiment analysis journey, whether you're analyzing social media sentiment or extracting insights from customer feedback.</p>

<h3>Complete code for config file</h3>
<pre>
    <code class="language-python">
        
import torch 

TRAIN_DATA_PATH = "./Raw_Data/train.csv"
VAL_DATA_PATH = "./Raw_Data/valid.csv"
TEST_DATA_PATH = "./Raw_Data/test.csv"
PRETRAINED_MODEL_PATH="camembert-base"
MAX_LENGTH = 130
BATCH_SIZE = 8

RESULT_PATH = "simple_test.txt"

FINAL_TRAIN_MODEL_PATH = "FINAL_TRAIN_MODEL.h5"

def device_GPU_CPU():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Thre are {torch.cuda.device_count()} GPU(s) available." )
        print('Device name:', torch.cuda.get_device_name(0))
    else:
        print('No GPU available, using the CPU instead.')
        device = torch.device("cpu")
    
    return device

    </code>
</pre>
</div>

            </div>
        </div>

        <div class="accordion-item">
            <h1 class="accordion-header" id="heading-2">
                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#dataPreparation" aria-expanded="true" aria-controls="dataPreparation" style="background-color:#7C81AD;">
                    Data preparation with Python: A step–by-step Guide:
                </button>
            </h1>
            <div id="dataPreparation" class="accordion-collapse collapse show" aria-labelledby="heading-2" data-bs-parent="#frenchSentimentAnalysis">
                <div class="accordion-body">
                    <p>Data preparation is a critical step in any data analysis or machine learning project. In this article, we will walk you through a python code snippet that demonstrates how to preprocess data from a csv file, specifically text data, using the pandas and numpy libraries. The goal is to clean and organize the data for further analysis or machine learning tasks.</p>  
                    <h3>Step 1: Importing Libraries</h3>
                    <pre>
<code class="language-python">
import pandas as pd
import numpy as np
</code>
                    </pre>

                    <p>The code begins by importing two essential libraries: pandas and numpy. Pandas is used for data manipulation and analysis, while numpy is used for numerical operations on data arrays.</p>
                    <h3>Step 2: Defining a Data Preprocessing Function</h3>
                    <pre>
<code class="language-python">
def data_preprocess(path: str):
</code>
                    </pre>



                    <p>A function named data_preprocess is defined to encapsulate the data preparation steps. It takes a single argument, ‘path’ which should be a string representing the path to a CSV file containing data to be processed.</p>
                    <h3>Step 3: Reading Data from CSV</h3>
                    <pre>
<code class="language-python">
raw_data = pd.read_csv(path, encoding="utf-8")
</code>
                    </pre>

                    <p>The first data processing step involves reading the data from the specified CSV file. The ‘pd.read_csv’ function from ‘pandas’ is used for this purpose. It reads the data and assumes that the CSV file is encoded in UTF-8.</p>
                        
                    <h3>Step 4: Selecting Relevant Columns</h3>
                    <pre>
<code class="language-python">
data = raw_data[["review", "polarity"]]
</code>
                    </pre>
                    <p>In this step, we select only the relevant columns from the raw data. 
                        Specifically, we choose the “review” column, which contains textual data and the “polarity” column, which represents labels or sentiment scores associated with the text. 
                        </p>

                <h3>Data cleaning</h3>
                <pre>
<code class="language-python">
data = data.dropna()
</code>
                </pre>
                <p>Data cleaning is an essential part of data preprocessing. The line of code removes rows with missing values (NaN) from the ‘data’ DataFrame. Cleaning the data ensures that it’s free from incomplete or unreliable information. </p>

                <h3>Step 6: Splitting Data into Features and Labels</h3>


                <pre>
<code class="language-python">
X = np.array(data['review'])
y = np.array(data['polarity'])
</code>
                </pre>

                <p>After cleaning the data, we proceed to split it into input features (X) and target labels(y). The “review” column becomes our input features, and the “polarity” column becomes our target label. These are converted into numpy array, which are more suitable for further processing. </p>
               
            <h3>Step 7: Returning processed Data</h3>
            <pre>
<code class="language-python">
return X, y
</code>
            </pre>

            <p>Finally, the data_preprocess function returns the processed data as a tuple: X, which contains the textual reviews, and y, which contains the corresponding polarity labels. This preprocessed data is now ready for analysis, machine learning, or any other tasks you have in mind.</p>
           
        
            <p>In summary, this Python code snippet demonstrates a basic yet crucial data preparation process for text data. It covers data loading, column selection, cleaning, and data splitting, setting the stage for various data analysis or machine learning applications.</p>
        
        
            <strong>Here is the complete code for data processing</strong>
<pre>
<code class="language-python">
import pandas as pd
import numpy as np 

def data_preprocess(path:str):
        raw_data = pd.read_csv(path,encoding="utf-8")
        data = raw_data[["review", "polarity"]]
        data = data.dropna()
        # data = data.sample(frac=0.01)

        X = np.array(data['review'])
        y = np.array(data['polarity'])

        return X,y

</code>
</pre>


        </div>

            </div>
        </div>


        <div class="accordion-item">
            <h1 class="accordion-header" id="heading-3">
                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#tokenization" aria-expanded="true" aria-controls="introduction" style="background-color:#7C81AD;">
                    Tokenization for Natural Language Processing with Camembert
                </button>
            </h1>
            <div id="tokenization" class="accordion-collapse collapse show" aria-labelledby="heading-3" data-bs-parent="#frenchSentimentAnalysis">
                <div class="accordion-body">
                    <h3>Introduction</h3>
                    <p>Tokenization is the process of converting raw text data into a format suitable for natural language processing (NLP) tasks. In this article, we explore a Python code snippet that demonstrates how to tokenize text data using the Camembert model from the Hugging Face Transformers library. This essential step is the foundation for a wide range of NLP tasks, from sentiment analysis to text generation.</p>
                    
                    <h3>Code Explanation</h3>
                    <pre>
<code class="language-python">
from transformers import CamembertTokenizer
import torch 
from config import PRETRAINED_MODEL_PATH
</code>
                    </pre>
                    <p>The code begins by importing the necessary libraries and dependencies, including the CamembertTokenizer, PyTorch, and a custom configuration file (config.py) that specifies the path to the pre-trained model.</p>
                    <h3>Function: tokenization(data, max_length)</h3>
<pre>
<code class="language-python">
def tokenization(data, max_length):
        print("Starting tokenization...")

        tokenizer = CamembertTokenizer.from_pretrained(PRETRAINED_MODEL_PATH)
        input_ids = []
        attention_mask = []
    </code>
</pre>

<p>This function tokenizes a list of text data (data) and has a parameter max_length that defines the maximum token length for the output. Inside the function, it initializes a Camembert tokenizer and empty lists to store the tokenized data.</p>
     <pre>
<code class="language-python">
# Encoding every sentence
for element in data:
    encoded_element = tokenizer.encode_plus(str(element), add_special_tokens=True, 
                                            truncation=True, max_length=max_length,
                                            padding='max_length', return_tensors='pt')
    input_ids.append(encoded_element["input_ids"])
    attention_mask.append(encoded_element["attention_mask"])

</code>
     </pre>   
     <p>The code iterates through each element in the input data, tokenizing them with the Camembert tokenizer. It uses encode_plus to handle the tokenization process, which includes adding special tokens, truncating or padding to meet the specified max_length, and returning the result as PyTorch tensors.</p>

     <pre>
<code class="language-python">

print("Tokenization finished!")

input_ids = torch.cat(input_ids, dim=0)
attention_mask = torch.cat(attention_mask, dim=0)
return input_ids, attention_mask

</code>
     </pre>

<p>The tokenized data is stored in the input_ids and attention_mask lists. After tokenizing all elements, the function concatenates them into PyTorch tensors and returns them. These tensors are now ready to be used as input for Camembert or other NLP models.</p>

<h3>Function: single_tokenizer(element, max_length)</h3>
<p>This is a similar function, but it's designed to tokenize a single text element, unlike the previous function that processes a list of data.</p>
<pre>
    <code class="language-python">
def single_tokenizer(element, max_length):
    tokenizer = CamembertTokenizer.from_pretrained(PRETRAINED_MODEL_PATH)
    encoded_element = tokenizer.encode_plus(str(element), add_special_tokens=True, 
    truncation=True, max_length=max_length,
    padding='max_length', return_tensors='pt')
    print("Tokenization finished!")

    return encoded_element["input_ids"], encoded_element["attention_mask"]

    </code>
</pre>
<p>It tokenizes the given text element and returns the input IDs and attention mask as PyTorch tensors.</p>

<h3>Conclusion</h3>
<p>In the field of natural language processing, tokenization is the first crucial step in preparing text data for various NLP tasks. This code snippet, utilizing the Camembert model from Hugging Face Transformers, demonstrates how to tokenize text data efficiently. These tokenized representations can then be used for tasks like sentiment analysis, text classification, and more, bringing us one step closer to understanding and extracting insights from human language.</p>

<h3>Complete code for tokenization</h3>
<pre>
    <code class="language-python">
from transformers import CamembertTokenizer
import torch 
from config import PRETRAINED_MODEL_PATH


# The below function tokenizes the data and it has two argupments

def tokenization(data, max_length):

    print("Starting tokenization...")

    tokenizer = CamembertTokenizer.from_pretrained(PRETRAINED_MODEL_PATH)
    input_ids = []
    attention_mask = []
    # Encoding every sentence
    for element in data:
        encoded_element = tokenizer.encode_plus(str(element), add_special_tokens=True, 
                                                truncation=True,max_length=max_length,
                                                padding='max_length', return_tensors='pt')
        input_ids.append(encoded_element["input_ids"])
        attention_mask.append(encoded_element["attention_mask"])
    print("Tokenization finished!")


    input_ids = torch.cat(input_ids,dim=0)
    attention_mask = torch.cat(attention_mask, dim=0)
    return input_ids, attention_mask

def single_tokenizer(element, max_length):
    tokenizer = CamembertTokenizer.from_pretrained(PRETRAINED_MODEL_PATH)
 
    encoded_element = tokenizer.encode_plus(str(element), add_special_tokens=True, 
                                                truncation=True,max_length=max_length,
                                                padding='max_length', return_tensors='pt')
    print("Tokenization finished!")
    # input_ids.append(encoded_element["input_ids"])
    # attention_mask.append(encoded_element["attention_mask"])
    return encoded_element["input_ids"], encoded_element["attention_mask"]
   

    </code>
</pre>
</div>

            </div>
        </div>

        <div class="accordion-item">
            <h1 class="accordion-header" id="heading-4">
                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#classifier" aria-expanded="true" aria-controls="classifier" style="background-color:#7C81AD;">
                    Building an Intelligent Sentiment Classifier with Camembert and PyTorch
                </button>
            </h1>
            <div id="classifier" class="accordion-collapse collapse show" aria-labelledby="heading-4" data-bs-parent="#frenchSentimentAnalysis">
                <div class="accordion-body">
                 <h3>Introduction</h3>
                <p>  In the realm of natural language processing, building a powerful sentiment classifier is a fascinating challenge. This Python code snippet demonstrates how to construct an advanced sentiment classifier using Camembert, a cutting-edge transformer-based model, and PyTorch, a popular deep learning framework. This section will walk you through the intricacies of creating a Main Classifier neural network, tokenizing text data, and training the model for accurate sentiment analysis.</p>

                <h3>Code Explanation</h3>
                <pre>
<code class="language-python">
import torch.nn as nn 
import torch 
from transformers import CamembertModel
from transformers import AdamW, get_linear_schedule_with_warmup
from config import PRETRAINED_MODEL_PATH
</code>
</pre>

<p>The code begins by importing essential libraries and dependencies. torch.nn is used for defining neural network components, and the CamembertModel from the Transformers library is employed as the backbone of the sentiment classifier. AdamW is used as the optimizer, and get_linear_schedule_with_warmup is employed for dynamic learning rate scheduling.</p>
<h3>Defining the Main Classifier</h3>
<pre>
    <code class="language-python">
class MainClassifier(nn.Module):
    def __init__(self):
        super(MainClassifier, self).__init__()
        # Pretrained Model
        self.camembert = CamembertModel.from_pretrained(PRETRAINED_MODEL_PATH)

        # Hidden layer
        self.fc1 = nn.Linear(self.camembert.config.hidden_size, 200)
        # Dropout Regularizer
        self.dropout = nn.Dropout(p=0.2)
        # Batch normalization
        self.batchnorm = nn.BatchNorm1d(self.camembert.config.hidden_size)
        # Output layers
        self.out = nn.Linear(200, 2)


    </code>
</pre>

<p>In the MainClassifier class, a Camembert model is loaded and serves as the backbone. The model's output is passed through a hidden layer (self.fc1), followed by dropout regularization and batch normalization. The final output is produced by the self.out linear layer, which maps the features to two classes (positive and negative sentiments).</p>
       
<h3>Forward Pass</h3>
<pre>
    <code class="language-python">
def forward(self, input_ids, input_masks):
    camembert = self.camembert(input_ids, input_masks)
    camembert = camembert[0]
    x = camembert[:, 0, :]
    x = self.batchnorm(x)
    x = self.dropout(x)
    x = torch.tanh(self.fc1(x))
    output = self.out(x)
    return output
    </code>
</pre>

<p>The forward method defines the forward pass of the model. It processes the input data through the Camembert model, applies batch normalization and dropout, and passes the features through the hidden layer. The output represents the predicted sentiment.</p>
<h3>Model initialization</h3>
<pre>
    <code class="language-python">
def initialize_model(dataloader_train, device="cpu", epochs=3):
    classifier = MainClassifier()
    classifier.to(device)
    optimizer = AdamW(classifier.parameters(), lr=5e-5, eps=1e-8)
    total_steps = len(dataloader_train) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
    return classifier, optimizer, scheduler
    </code>
</pre>
<p>The initialize_model function initializes the Main Classifier model. It takes a training data loader, a device specification, and the number of training epochs as inputs. The function returns the initialized classifier, optimizer, and scheduler, ready for training.</p>

<h3>Conclusion</h3>
<p>In this section, we've explored the process of building a sentiment classifier using the Camembert transformer model and PyTorch. By delving into the Main Classifier architecture, tokenization techniques, and model initialization, we've laid the foundation for accurate sentiment analysis. Armed with this knowledge, you can venture further into the realm of natural language processing, opening doors to diverse applications such as chatbots, social media analysis, and customer feedback systems. Happy coding!</p>

<h3>Complete code for classifier</h3>
<pre>
    <code class="language-python">
        import torch.nn as nn 
import torch 
from transformers import CamembertModel
from transformers import AdamW, get_linear_schedule_with_warmup
from config import PRETRAINED_MODEL_PATH

class MainClassifier(nn.Module):
    def __init__(self):
        super(MainClassifier, self).__init__()
        # Pretrained Model
        self.camembert = CamembertModel.from_pretrained(PRETRAINED_MODEL_PATH)

        # Hidden layer
        self.fc1 = nn.Linear(self.camembert.config.hidden_size, 200)
        # Dropout Regularizer
        self.dropout = nn.Dropout(p=0.2)
        # Batch normalization
        self.batchnorm = nn.BatchNorm1d(self.camembert.config.hidden_size)
        # Output layers
        self.out = nn.Linear(200,2)


    def forward(self,input_ids, input_masks):
        camembert = self.camembert(input_ids, input_masks)
        camembert = camembert[0]
        x = camembert[:, 0, :]
        x = self.batchnorm(x)
        x = self.dropout(x)
        x = torch.tanh(self.fc1(x))
        output = self.out(x)
        return output
    

# Initialization of the model 
def initialize_model(dataloader_train, device="cpu", epochs=3):
    classifier = MainClassifier()
    classifier.to(device)
    optimizer = AdamW(classifier.parameters(), lr=5e-5, eps=1e-8)
    total_steps  = len(dataloader_train) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
    return classifier, optimizer, scheduler



    </code>
</pre>
</div>

            </div>
        </div>

        <div class="accordion-item">
            <h1 class="accordion-header" id="heading-5">
                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#models" aria-expanded="true" aria-controls="models" style="background-color:#7C81AD;">
                    Building a Robust Sentiment Analysis Model with PyTorch and Transformers
                </button>
            </h1>
            <div id="models" class="accordion-collapse collapse show" aria-labelledby="heading-5" data-bs-parent="#frenchSentimentAnalysis">
                <div class="accordion-body">
                    <h3>Introduction</h3>
                    <p>This code takes you through the steps of data preprocessing, model training, and performance evaluation.</p>
                    <h3>Code Explanation</h3>
                    <pre>
<code class="language-python">
# Import Libraries and Packages
from numpy.core.numeric import False_
import pandas as pd 
import numpy as np 
import torch 
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
import torch.nn as nn 
import torch.nn.functional as F
from sklearn metrics import roc_auc_score, classification_report, confusion_matrix
import random 
import time

from classifier import initialize_model
from tokenizer_local import tokenization
from config import MAX_LENGTH, BATCH_SIZE, TRAIN_DATA_PATH, VAL_DATA_PATH, device_GPU_CPU, FINAL_TRAIN_MODEL_PATH
from data_prepocess import data_preprocess
torch.cuda.empty_cache()
# Configuration of device
device = device_GPU_CPU()
</code>
                    </pre>

                    <p>This section begins by importing the necessary libraries and packages, including pandas for data handling, numpy for numerical operations, torch for PyTorch, and various other components. It also imports custom modules such as classifier, tokenizer_local, and configuration settings from config.py.</p>

<pre>
<code class="language-python">
loss_fn = nn.CrossEntropyLoss()
</code>
</pre>


            <p>Here, the code sets the loss function to CrossEntropyLoss, which is commonly used for classification tasks.</p>
<pre>
<code class="language-python">
def set_seed(seed_value=42):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)

</code>
</pre>

<p>The set_seed function ensures that the random number generation is reproducible by setting random seeds.</p>
<pre>
    <code class="language-python">
        def evaluate(model, val_dataloader, labels_test):
            # ... (see next explanation)
    
    </code>
    </pre>
            
    <p>The evaluate function is used to evaluate the model's performance on a validation dataset, calculating loss and accuracy.</p>

<pre>
<code class="language-python">
def train(model, train_dataloader, optimizer, scheduler, labels_test, val_dataloader=None, epochs=3, evaluation=False):
    # ... (see next explanation)

</code>
    </pre>
<p>The train function is responsible for training the sentiment analysis model, including forward and backward passes, optimization, and optional evaluation.</p>

    <pre>
        <code class="language-python">
def all_metrics(preds, labels):
    # ... (see next explanation)

        </code>
    </pre>
    <p>The all_metrics function computes performance metrics, such as classification reports and confusion matrices, based on model predictions and true labels.</p>
              
    <pre>
        <code class="language-python">

def predict(model, test_dataloader):
        # ... (see next explanation)
        
        </code>
    </pre>
    <p>The predict function uses the trained model to make predictions on a test dataset, returning class probabilities</p>
    <pre>
        <code class="language-python">
def execute():
    # ... (see next explanation)

        </code>
    </pre>

    <p>The execute function brings all the components together, from data preprocessing to training and evaluation. It loads the data, tokenizes it, initializes the model, trains the model, and evaluates its performance, showcasing essential steps in building a sentiment analysis system.</p>
    <h3>Conclusion</h3>
    <p>Sentiment analysis is a vital tool for extracting valuable insights from textual data. In this article, we've explored a comprehensive code snippet for building a sentiment analysis model using PyTorch and Transformers. This code demonstrates the entire pipeline, from data preparation to model training and evaluation. By following this example, you can create powerful sentiment analysis models for a wide range of applications, from social media sentiment tracking to customer feedback analysis.</p>


    <h3>Here is the complete code of this section</h3>
    <pre>
        <code class="language-python">
            # Import Librairies and packages

from numpy.core.numeric import False_
import pandas as pd 
import numpy as np 
import torch 
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
import torch.nn as nn 
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import random 
import time

from classifier import initialize_model
from tokenizer_local import tokenization
from config import MAX_LENGTH, BATCH_SIZE,TRAIN_DATA_PATH, VAL_DATA_PATH,device_GPU_CPU,FINAL_TRAIN_MODEL_PATH
from data_prepocess import data_preprocess
torch.cuda.empty_cache()
# Configuration of device
device = device_GPU_CPU()

loss_fn = nn.CrossEntropyLoss()

def set_seed(seed_value=42):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)


def evaluate(model, val_dataloader,labels_test):
    model.eval()
    # Tracking the variables
    val_accuracy = []
    val_loss = []
    all_logits = []
    # For each batch in our validation set:
    for batch in val_dataloader:
        # Load bathc to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)
        
        all_logits.append(logits)
        # Compute loss 
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds =torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds==b_labels).cpu().numpy().mean() *100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)
    all_logits= torch.cat(all_logits, dim=0)
    
    # probabilities 
    probs = F.softmax(all_logits, dim=1).cpu().numpy()
    labels_test = labels_test.ravel()
    
    # AUC of multiclass

    # roc_auc = roc_auc_score(labels_test,probs,multi_class='ovr')
    return val_loss, val_accuracy#,roc_auc

def train(model, train_dataloader, optimizer, scheduler, labels_test, val_dataloader=None, epochs=3, evaluation=False):
    print("Start training...\n")
    for epoch_i in range(epochs):
        # Print the header of the result table
        print(f"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'AUC':^9} | {'Elapsed':^9}")
        print("-"*70)
        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0.0, 0, 0

        # Put the model into the training mode
        model.train()
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)
            b_labels = b_labels.squeeze()
            # Zero out any previously calculated gradients
            model.zero_grad()
            # Perform a forward pass. This will return logits.
           
            logits = model(b_input_ids, b_attn_mask)
            # Compute loss and accumulate the loss values
            loss = loss_fn(logits.squeeze(), b_labels)
            batch_loss += loss.item()
            total_loss += loss.item()
            # Perform a backward pass to calculate gradients
            loss.backward()
            # Clip the norm of the gradients to 1.0 to prevent "exploding gradients"
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 1000 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 10000 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                print(f"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {'-':^9} | {time_elapsed:^9.2f}")

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()
            # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        print("-"*70)

        # =======================================
        #               Evaluation Section
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy= evaluate(model, val_dataloader, labels_test)
            min_loss = np.inf
            if val_loss < min_loss:
                min_loss = val_loss
                # Saving the model
                torch.save(model.state_dict(),FINAL_TRAIN_MODEL_PATH)
                print("model saved")
            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch 
            print(f"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}")
            print("-"*70)
        print("\n")
    #torch.save(model.state_dict(),path_modele)
    print("Training complete!")

# Performance metrics

def all_metrics(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return classification_report(labels_flat, preds_flat), confusion_matrix(labels_flat, preds_flat)

# Prediction de validation

def predict(model, test_dataloader):
    # Put the model into the evaluation mode. The dropout layers are disabled during
    # the test time.
    model.eval()

    all_logits = []

    # For each batch in our test set...
    for batch in test_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)
        all_logits.append(logits)
    # Concatenate logits from each batch
    all_logits = torch.cat(all_logits, dim=0)

    # Apply softmax to calculate probabilities
    probs = F.softmax(all_logits, dim=1).cpu().numpy()

    return probs
# Performance metrics

def all_metrics(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return classification_report(labels_flat, preds_flat), confusion_matrix(labels_flat, preds_flat)

# Prediction de validation

def predict(model, test_dataloader):
    # Put the model into the evaluation mode. The dropout layers are disabled during
    # the test time.
    model.eval()

    all_logits = []

    # For each batch in our test set...
    for batch in test_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)
        all_logits.append(logits)
    # Concatenate logits from each batch
    all_logits = torch.cat(all_logits, dim=0)

    # Apply softmax to calculate probabilities
    probs = F.softmax(all_logits, dim=1).cpu().numpy()

    return probs



# =======================================
#           Section d'execution
# =======================================

def execute():
    
    X_train, y_train= data_preprocess(TRAIN_DATA_PATH)
    X_test, y_test = data_preprocess(VAL_DATA_PATH)
    # Tokenizing training data 
    input_ids_train, attention_masks_train = tokenization(X_train, MAX_LENGTH)
    labels_train = torch.tensor(y_train)
    # Tokenizing testing data 
    input_ids_test, attention_masks_test = tokenization(X_test, MAX_LENGTH)
    labels_test = torch.tensor(y_test)
    # Train dataset
    dataset_train = TensorDataset(input_ids_train,attention_masks_train,labels_train)
    # Test dataset
    dataset_test= TensorDataset(input_ids_test, attention_masks_test,labels_test)
    dataloader_train =  DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=BATCH_SIZE)
    dataloader_val =  DataLoader(dataset_test, sampler=SequentialSampler(dataset_test), batch_size=BATCH_SIZE)
    set_seed(42)    # Set seed for reproducibility
    classifier, optimizer, scheduler = initialize_model(dataloader_train, device, epochs=3)
    train(classifier, dataloader_train, optimizer, scheduler, labels_test, dataloader_val, epochs=3, evaluation=True)
    probs = predict(classifier, dataloader_val)
    classification, matriceP = all_metrics(probs, labels_test)
    print("______________classification report______________________")
    print(classification)
    print("______________ Confusion matrix_______________________")
    print(matriceP)





        </code>
    </pre>
</div>

            </div>
        </div>



    </div>

</div>







<!-- <div class="accordion-item">
    <h2 class="accordion-header" id="heading-1">
        <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#introduction" aria-expanded="true" aria-controls="introduction" style="background-color:#7C81AD;">
           Introduction
        </button>
    </h2>
    <div id="introduction" class="accordion-collapse collapse show" aria-labelledby="heading-1" data-bs-parent="#aboutTeam">
        <div class="accordion-body">
        
        </div>

    </div>
</div> -->